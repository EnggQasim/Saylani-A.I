{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The pandas DataFrame Object<br>\n",
    "The pandas DataFrame object extends the capabilities of the Series object into\n",
    "two-dimensions. A Series object adds an index to a NumPy array but can only\n",
    "associate a single data item per index label, a DataFrame integrates multiple Series\n",
    "objects by aligning them along common index labels. This automatic alignment by\n",
    "index label provides a seamless view across all the Series at each index label that\n",
    "has the appearance of a row in a table.<br><br>\n",
    "A DataFrame object can be thought of as a dictionary-like container of one or more\n",
    "Series objects, or as a spreadsheet, probably the best description for those new to\n",
    "pandas is to compare a DataFrame object to a relational database table. However,\n",
    "even that comparison is limited, as a DataFrame object has very distinct qualities\n",
    "(such as automatic data alignment of series) that make it much more capable for\n",
    "exploratory data analysis than either a spreadsheet or relational database table.<br><br>\n",
    "Because of the increased dimensionality of the DataFrame object, it becomes necessary\n",
    "to provide a means to select both rows and columns. Carrying over from a Series,\n",
    "the DataFrame uses the [] operator for selection, but it is now applied to the selection\n",
    "of columns of data. This means that another construct must be used to select specific\n",
    "rows of a DataFrame object. For those operations, a DataFrame object provides several\n",
    "methods and attributes that can be used in various fashions to select data by rows.<br><br>\n",
    "A DataFrame also introduces the concept of multiple axes, specifically the horizontal\n",
    "and vertical axis. Functions from pandas can then be applied to either axis, in essence\n",
    "stating that the operation be applied horizontally to all the values in the rows, or up\n",
    "and down each column.<br><br>\n",
    "In this chapter, we will examine the pandas DataFrame and how we can manipulate\n",
    "both the DataFrame and the data it represents to build a basis for performing\n",
    "interactive data analysis.\n",
    "## we will cover:\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "* Creating a DataFrame from scratch\n",
    "• Loading sample data to demonstrate the capabilities of a DataFrame object\n",
    "• Selecting columns of a DataFrame object\n",
    "• Selecting rows and values of a DataFrame using the index\n",
    "• Selecting rows of a DataFrame using Boolean selection\n",
    "• Adding, replacing, and deleting columns from a DataFrame\n",
    "• Adding, replacing, and deleting rows from a DataFrame\n",
    "• Modifying scalar values in a DataFrame\n",
    "• Arithmetic operations on the DataFrame objects\n",
    "• Resetting and reindexing a DataFrame\n",
    "• Hierarchically indexing a DataFrame\n",
    "• Statistical methods of a DataFrame\n",
    "• Summarized data and statistical methods of a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DataFrame from scratch\n",
    "To use a DataFrame we first need to import pandas and set some options for output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reference NumPy and pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Set some pandas options\n",
    "pd.set_option('display.notebook_repr_html', False)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    0   1\n",
       "0  10  11\n",
       "1  20  21"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.array([[10, 11], [20, 21]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    0   1   2   3   4\n",
       "0  10  11  12  13  14\n",
       "1  15  16  17  18  19"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a DataFrame for a list of Series objects\n",
    "df1 = pd.DataFrame([pd.Series(np.arange(10, 15)),\n",
    "                    pd.Series(np.arange(15, 20))])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what's the shape of this DataFrame\n",
    "df1.shape # it is two rows by 5 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    a   b\n",
       "0  10  11\n",
       "1  20  21"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify column names\n",
    "df = pd.DataFrame(np.array([[10, 11], [20, 21]]), columns=['a', 'b'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['a', 'b'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what are the names of the columns?\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a, b'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve just the names of the columns by position\n",
    "\"{0}, {1}\".format(df.columns[0], df.columns[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   c1  c2\n",
       "0  10  11\n",
       "1  20  21"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename the columns\n",
    "df.columns = ['c1', 'c2']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    c1  c2\n",
       "r1   0   1\n",
       "r2   2   3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a DataFrame with named columns and rows\n",
    "df = pd.DataFrame(np.array([[0, 1], [2, 3]]),\n",
    "                  columns=['c1', 'c2'],\n",
    "                  index=['r1', 'r2'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['r1', 'r2'], dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve the index of the DataFrame\n",
    "df.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   c1  c2\n",
       "0   1   6\n",
       "1   2   7\n",
       "2   3   8\n",
       "3   4   9\n",
       "4   5  10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a DataFrame with two Series objects\n",
    " # and a dictionary\n",
    "s1 = pd.Series(np.arange(1, 6, 1))\n",
    "s2 = pd.Series(np.arange(6, 11, 1))\n",
    "pd.DataFrame({'c1': s1, 'c2': s2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   c1  c2    c3\n",
       "0   1   6   NaN\n",
       "1   2   7  12.0\n",
       "2   3   8  13.0\n",
       "3   4   9   NaN\n",
       "4   5  10   NaN"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# demonstrate alignment during creation\n",
    "s3 = pd.Series(np.arange(12, 14), index=[1, 2])\n",
    "df = pd.DataFrame({'c1': s1, 'c2': s2, 'c3': s3})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example data\n",
    "Where possible, the examples in this chapter will utilize several datasets provided\n",
    "with the code in the download for the text. These datasets make the examples a little\n",
    "less academic in nature. These datasets will be read from files using the <b>pd.read_\n",
    "csv()</b> function that will load the sample data from the file into a DataFrame object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'head' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# show the first three lines of the file\n",
    "!head -n 3 data\\prices.csv # on Mac or Linux\n",
    " # !type data\\sp500.csv # on Windows, but will show the entire file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                           open       close     volume\n",
       "date                                                  \n",
       "2016-01-05 00:00:00  123.430000  125.839996  2163600.0\n",
       "2016-01-06 00:00:00  125.239998  119.980003  2386400.0\n",
       "2016-01-07 00:00:00  116.379997  114.949997  2489500.0\n",
       "2016-01-08 00:00:00  115.480003  116.620003  2006300.0\n",
       "2016-01-11 00:00:00  117.010002  114.970001  1408600.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the data and print the first five rows\n",
    " # use the Symbol column as the index, and\n",
    " # only read in columns in positions 0, 2, 3, 7\n",
    "sp500 = pd.read_csv(\"data/prices.csv\",\n",
    "                    index_col='date',\n",
    "                    usecols=[0, 2, 3, 6])\n",
    "sp500.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                           open       close     volume\n",
       "date                                                  \n",
       "2016-12-30           103.309998  103.199997   973800.0\n",
       "2016-12-30            43.070000   43.040001  1938100.0\n",
       "2016-12-30            53.639999   53.529999  1701200.0\n",
       "2016-12-30 00:00:00   44.730000   45.450001  1380900.0\n",
       "2016-12-30 00:00:00   54.200001   53.630001   705100.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp500.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "851264"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sp500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['2016-01-05 00:00:00', '2016-01-06 00:00:00', '2016-01-07 00:00:00',\n",
       "       '2016-01-08 00:00:00', '2016-01-11 00:00:00', '2016-01-12 00:00:00',\n",
       "       '2016-01-13 00:00:00', '2016-01-14 00:00:00', '2016-01-15 00:00:00',\n",
       "       '2016-01-19 00:00:00',\n",
       "       ...\n",
       "       '2016-12-30', '2016-12-30', '2016-12-30', '2016-12-30', '2016-12-30',\n",
       "       '2016-12-30', '2016-12-30', '2016-12-30', '2016-12-30 00:00:00',\n",
       "       '2016-12-30 00:00:00'],\n",
       "      dtype='object', name='date', length=851264)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp500.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['open', 'close', 'volume'], dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp500.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!type data\\prices.csv # on Windows, but prints the entire file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                  date symbol        open       close         low        high  \\\n",
       "0  2016-01-05 00:00:00   WLTW  123.430000  125.839996  122.309998  126.250000   \n",
       "1  2016-01-06 00:00:00   WLTW  125.239998  119.980003  119.940002  125.540001   \n",
       "2  2016-01-07 00:00:00   WLTW  116.379997  114.949997  114.930000  119.739998   \n",
       "\n",
       "      volume  \n",
       "0  2163600.0  \n",
       "1  2386400.0  \n",
       "2  2489500.0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the data\n",
    "one_mon_hist = pd.read_csv(\"data/prices.csv\")\n",
    "# examine the first three rows\n",
    "one_mon_hist[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting columns of a DataFrame\n",
    "Selecting the data in specific columns of a DataFrame is performed by using the []\n",
    "operator. This can be passed either as a single object, or a list of objects. These objects\n",
    "are then used to lookup columns either by zero-based location, or by matching the\n",
    "objects to the values in the columns index.\n",
    "Passing a single integer, or a list of integers, to [] will have the DataFrame object\n",
    "attempt to perform a location based lookup of the columns. The following code\n",
    "retrieves the data in the second and third columns:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                           open       close\n",
       "date                                       \n",
       "2016-01-05 00:00:00  123.430000  125.839996\n",
       "2016-01-06 00:00:00  125.239998  119.980003\n",
       "2016-01-07 00:00:00  116.379997  114.949997\n",
       "2016-01-08 00:00:00  115.480003  116.620003\n",
       "2016-01-11 00:00:00  117.010002  114.970001"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get first and second columns (1 and 2) by location\n",
    "sp500[['open', 'close']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                           open\n",
       "date                           \n",
       "2016-01-05 00:00:00  123.430000\n",
       "2016-01-06 00:00:00  125.239998\n",
       "2016-01-07 00:00:00  116.379997\n",
       "2016-01-08 00:00:00  115.480003\n",
       "2016-01-11 00:00:00  117.010002"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp500[['open']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sp500[['open']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # this is an exception, hence it is commented\n",
    " # this tries to find a column named '1'\n",
    " # not the row at position 1\n",
    " # df = sp500[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                              0           1          2\n",
       "date                                                  \n",
       "2016-01-05 00:00:00  123.430000  125.839996  2163600.0\n",
       "2016-01-06 00:00:00  125.239998  119.980003  2386400.0\n",
       "2016-01-07 00:00:00  116.379997  114.949997  2489500.0\n",
       "2016-01-08 00:00:00  115.480003  116.620003  2006300.0\n",
       "2016-01-11 00:00:00  117.010002  114.970001  1408600.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a new DataFrame with integers as the column names\n",
    " # make sure to use .copy() or change will be in-place\n",
    "df = sp500.copy()\n",
    "df.columns=[0, 1, 2]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date\n",
       "2016-01-05 00:00:00    125.839996\n",
       "2016-01-06 00:00:00    119.980003\n",
       "2016-01-07 00:00:00    114.949997\n",
       "2016-01-08 00:00:00    116.620003\n",
       "2016-01-11 00:00:00    114.970001\n",
       "                          ...    \n",
       "2016-12-30             103.199997\n",
       "2016-12-30              43.040001\n",
       "2016-12-30              53.529999\n",
       "2016-12-30 00:00:00     45.450001\n",
       "2016-12-30 00:00:00     53.630001\n",
       "Name: 1, Length: 851264, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([0, 1, 2], dtype='int64')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# because the column names are actually integers\n",
    " # and therefore [1] is found as a column\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is a Series not a DataFrame\n",
    "type(df[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date\n",
       "2016-01-05 00:00:00    123.430000\n",
       "2016-01-06 00:00:00    125.239998\n",
       "2016-01-07 00:00:00    116.379997\n",
       "2016-01-08 00:00:00    115.480003\n",
       "2016-01-11 00:00:00    117.010002\n",
       "                          ...    \n",
       "2016-12-30             103.309998\n",
       "2016-12-30              43.070000\n",
       "2016-12-30              53.639999\n",
       "2016-12-30 00:00:00     44.730000\n",
       "2016-12-30 00:00:00     54.200001\n",
       "Name: open, Length: 851264, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get price column by name\n",
    " # result is a Series\n",
    "sp500['open']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                           open       close\n",
       "date                                       \n",
       "2016-01-05 00:00:00  123.430000  125.839996\n",
       "2016-01-06 00:00:00  125.239998  119.980003\n",
       "2016-01-07 00:00:00  116.379997  114.949997\n",
       "2016-01-08 00:00:00  115.480003  116.620003\n",
       "2016-01-11 00:00:00  117.010002  114.970001\n",
       "...                         ...         ...\n",
       "2016-12-30           103.309998  103.199997\n",
       "2016-12-30            43.070000   43.040001\n",
       "2016-12-30            53.639999   53.529999\n",
       "2016-12-30 00:00:00   44.730000   45.450001\n",
       "2016-12-30 00:00:00   54.200001   53.630001\n",
       "\n",
       "[851264 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get Price and Sector columns\n",
    " # since a list is passed, the result is a DataFrame\n",
    "sp500[['open', 'close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date\n",
       "2016-01-05 00:00:00    123.430000\n",
       "2016-01-06 00:00:00    125.239998\n",
       "2016-01-07 00:00:00    116.379997\n",
       "2016-01-08 00:00:00    115.480003\n",
       "2016-01-11 00:00:00    117.010002\n",
       "                          ...    \n",
       "2016-12-30             103.309998\n",
       "2016-12-30              43.070000\n",
       "2016-12-30              53.639999\n",
       "2016-12-30 00:00:00     44.730000\n",
       "2016-12-30 00:00:00     54.200001\n",
       "Name: open, Length: 851264, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attribute access of the column by name\n",
    "sp500.open"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Note that this will not work for the Book Value column, as the name has a space.\n",
    "If you do want to find the zero-based location of one or more columns using the\n",
    "name of the column (technically, the value of the index entry of a column), use the\n",
    ".get_loc() method of the columns index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the position of the column with the value of Price\n",
    "loc = sp500.columns.get_loc('close')\n",
    "loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting rows and values of a DataFrame using the index"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Row selection using the index on a DataFrame then breaks down to the following\n",
    "general categories of operations:\n",
    "• Slicing using the [] operator\n",
    "• Label or location based lookup using .loc, .iloc, and .ix\n",
    "• Scalar lookup by label or location using .at and .iat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing using the [] operator\n",
    "Slicing a DataFrame across its index is syntactically identical to performing the\n",
    "same on a Series. Because of this, we will not go into the details of the various\n",
    "permutations of slices in this section, and only give representative examples\n",
    "applied to a DataFrame.<br><br>\n",
    "Slicing works along both positions and labels. The following code demonstrates\n",
    "several examples of slicing by position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                           open       close     volume\n",
       "date                                                  \n",
       "2016-01-05 00:00:00  123.430000  125.839996  2163600.0\n",
       "2016-01-06 00:00:00  125.239998  119.980003  2386400.0\n",
       "2016-01-07 00:00:00  116.379997  114.949997  2489500.0\n",
       "2016-01-08 00:00:00  115.480003  116.620003  2006300.0\n",
       "2016-01-11 00:00:00  117.010002  114.970001  1408600.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first five rows\n",
    "sp500[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                  date symbol        open       close         low        high  \\\n",
       "0  2016-01-05 00:00:00   WLTW  123.430000  125.839996  122.309998  126.250000   \n",
       "1  2016-01-06 00:00:00   WLTW  125.239998  119.980003  119.940002  125.540001   \n",
       "2  2016-01-07 00:00:00   WLTW  116.379997  114.949997  114.930000  119.739998   \n",
       "3  2016-01-08 00:00:00   WLTW  115.480003  116.620003  113.500000  117.440002   \n",
       "4  2016-01-11 00:00:00   WLTW  117.010002  114.970001  114.089996  117.330002   \n",
       "\n",
       "      volume  \n",
       "0  2163600.0  \n",
       "1  2386400.0  \n",
       "2  2489500.0  \n",
       "3  2006300.0  \n",
       "4  1408600.0  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp500 = pd.read_csv(\"data/prices.csv\")\n",
    "sp500.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                   date symbol        open       close         low  \\\n",
       "5   2016-01-12 00:00:00   WLTW  115.510002  115.550003  114.500000   \n",
       "6   2016-01-13 00:00:00   WLTW  116.459999  112.849998  112.589996   \n",
       "7   2016-01-14 00:00:00   WLTW  113.510002  114.379997  110.050003   \n",
       "8   2016-01-15 00:00:00   WLTW  113.330002  112.529999  111.919998   \n",
       "9   2016-01-19 00:00:00   WLTW  113.660004  110.379997  109.870003   \n",
       "10  2016-01-20 00:00:00   WLTW  109.059998  109.300003  108.320000   \n",
       "\n",
       "          high     volume  \n",
       "5   116.059998  1098000.0  \n",
       "6   117.070000   949600.0  \n",
       "7   115.029999   785300.0  \n",
       "8   114.879997  1093700.0  \n",
       "9   115.870003  1523500.0  \n",
       "10  111.599998  1653900.0  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp500[5:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting rows by index label and location: `.loc[] and .iloc[]`\n",
    "Rows can be retrieved via an index label value using .loc[]. This is shown in the\n",
    "following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date      2016-01-12 00:00:00\n",
       "symbol                   WLTW\n",
       "open                   115.51\n",
       "close                  115.55\n",
       "low                     114.5\n",
       "high                   116.06\n",
       "volume              1.098e+06\n",
       "Name: 5, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp500.loc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                  date symbol        open       close         low        high  \\\n",
       "5  2016-01-12 00:00:00   WLTW  115.510002  115.550003  114.500000  116.059998   \n",
       "7  2016-01-14 00:00:00   WLTW  113.510002  114.379997  110.050003  115.029999   \n",
       "\n",
       "      volume  \n",
       "5  1098000.0  \n",
       "7   785300.0  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp500.loc[[5,7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                  date symbol        open       close         low        high  \\\n",
       "1  2016-01-06 00:00:00   WLTW  125.239998  119.980003  119.940002  125.540001   \n",
       "2  2016-01-07 00:00:00   WLTW  116.379997  114.949997  114.930000  119.739998   \n",
       "\n",
       "      volume  \n",
       "1  2386400.0  \n",
       "2  2489500.0  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp500.iloc[[1,2]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "It is possible to look up the location in the index of a specific label value, which can\n",
    "then be used to retrieve the row(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5 2'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the location of MMM and A in the index\n",
    "i1 = sp500.index.get_loc(5)\n",
    "i2 = sp500.index.get_loc(2)\n",
    "\"{0} {1}\".format(i1, i2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                  date symbol        open       close     low        high  \\\n",
       "5  2016-01-12 00:00:00   WLTW  115.510002  115.550003  114.50  116.059998   \n",
       "2  2016-01-07 00:00:00   WLTW  116.379997  114.949997  114.93  119.739998   \n",
       "\n",
       "      volume  \n",
       "5  1098000.0  \n",
       "2  2489500.0  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp500.loc[[i1,i2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting rows by index label and/or\n",
    "location: .ix[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like a Series, a DataFrame also contains an .ix property that can be used to lookup\n",
    "rows, either by index label or location, essentially combining .loc and .iloc in one.\n",
    "The following looks up rows by index label by passing a list of nonintegers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muhammad Qasim\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                  date symbol        open       close         low        high  \\\n",
       "1  2016-01-06 00:00:00   WLTW  125.239998  119.980003  119.940002  125.540001   \n",
       "5  2016-01-12 00:00:00   WLTW  115.510002  115.550003  114.500000  116.059998   \n",
       "\n",
       "      volume  \n",
       "1  2386400.0  \n",
       "5  1098000.0  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp500.ix[[1,5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalar lookup by label or location using .at[] and .iat[]\n",
    "Scalar values can be looked up by label using .at, by passing both the row label and\n",
    "then the column name/value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115.510002"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# by label in both the index and column\n",
    "sp500.at[5, 'open']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scalar values can also be looked up by location using .iat by passing both the row\n",
    "location and then the column location. This is the preferred method of accessing\n",
    "single values and gives the highest performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WLTW'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# by location. Row 0, column 1\n",
    "sp500.iat[0, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting rows of a DataFrame by Boolean selection\n",
    "Rows can also be selected by using Boolean selection, using an array calculated from\n",
    "the result of applying a logical condition on the values in any of the columns. This\n",
    "allows us to build more complicated selections than those based simply upon index\n",
    "labels or positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         False\n",
       "1         False\n",
       "2         False\n",
       "3         False\n",
       "4         False\n",
       "          ...  \n",
       "851259    False\n",
       "851260     True\n",
       "851261     True\n",
       "851262     True\n",
       "851263     True\n",
       "Name: open, Length: 851264, dtype: bool"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what rows have a price < 100?\n",
    "sp500.open < 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                       date symbol       open      close        low  \\\n",
       "251              2010-01-04      A  31.389999  31.300001  31.130000   \n",
       "252              2010-01-04    AAL   4.840000   4.770000   4.660000   \n",
       "253              2010-01-04    AAP  40.700001  40.380001  40.360001   \n",
       "255              2010-01-04    ABC  26.290001  26.629999  26.139999   \n",
       "256              2010-01-04    ABT  54.189953  54.459951  53.919951   \n",
       "...                     ...    ...        ...        ...        ...   \n",
       "851258           2016-12-30    YUM  63.930000  63.330002  63.160000   \n",
       "851260           2016-12-30   ZION  43.070000  43.040001  42.689999   \n",
       "851261           2016-12-30    ZTS  53.639999  53.529999  53.270000   \n",
       "851262  2016-12-30 00:00:00    AIV  44.730000  45.450001  44.410000   \n",
       "851263  2016-12-30 00:00:00    FTV  54.200001  53.630001  53.389999   \n",
       "\n",
       "             high      volume  \n",
       "251     31.630001   3815500.0  \n",
       "252      4.940000   9837300.0  \n",
       "253     41.040001   1701700.0  \n",
       "255     26.690001   2455900.0  \n",
       "256     54.559954  10829000.0  \n",
       "...           ...         ...  \n",
       "851258  63.939999   1887100.0  \n",
       "851260  43.310001   1938100.0  \n",
       "851261  53.740002   1701200.0  \n",
       "851262  45.590000   1380900.0  \n",
       "851263  54.480000    705100.0  \n",
       "\n",
       "[721287 rows x 7 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now get the rows with Price < 100\n",
    "sp500[sp500.open < 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                       date symbol       open      close        low  \\\n",
       "251              2010-01-04      A  31.389999  31.300001  31.130000   \n",
       "252              2010-01-04    AAL   4.840000   4.770000   4.660000   \n",
       "253              2010-01-04    AAP  40.700001  40.380001  40.360001   \n",
       "255              2010-01-04    ABC  26.290001  26.629999  26.139999   \n",
       "256              2010-01-04    ABT  54.189953  54.459951  53.919951   \n",
       "...                     ...    ...        ...        ...        ...   \n",
       "851258           2016-12-30    YUM  63.930000  63.330002  63.160000   \n",
       "851260           2016-12-30   ZION  43.070000  43.040001  42.689999   \n",
       "851261           2016-12-30    ZTS  53.639999  53.529999  53.270000   \n",
       "851262  2016-12-30 00:00:00    AIV  44.730000  45.450001  44.410000   \n",
       "851263  2016-12-30 00:00:00    FTV  54.200001  53.630001  53.389999   \n",
       "\n",
       "             high      volume  \n",
       "251     31.630001   3815500.0  \n",
       "252      4.940000   9837300.0  \n",
       "253     41.040001   1701700.0  \n",
       "255     26.690001   2455900.0  \n",
       "256     54.559954  10829000.0  \n",
       "...           ...         ...  \n",
       "851258  63.939999   1887100.0  \n",
       "851260  43.310001   1938100.0  \n",
       "851261  53.740002   1701200.0  \n",
       "851262  45.590000   1380900.0  \n",
       "851263  54.480000    705100.0  \n",
       "\n",
       "[720358 rows x 7 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp500[(sp500.open<100) & (sp500.open>4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modifying the structure and content of DataFrame\n",
    "The structure and content of a DataFrame can be mutated in several ways. Rows and\n",
    "columns can be added and removed, and data within either can be modified to take\n",
    "on new values. Additionally, columns, as well as index labels, can also be renamed.\n",
    "Each of these will be described in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # rename the Book Value column to not have a space\n",
    " # this returns a copy with the column renamed\n",
    "df = sp500.rename(columns=\n",
    "                  {'open': 'price'})\n",
    " # print first 2 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                  date symbol       price       close         low        high  \\\n",
       "0  2016-01-05 00:00:00   WLTW  123.430000  125.839996  122.309998  126.250000   \n",
       "1  2016-01-06 00:00:00   WLTW  125.239998  119.980003  119.940002  125.540001   \n",
       "\n",
       "      volume  \n",
       "0  2163600.0  \n",
       "1  2386400.0  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has returned a new DataFrame object with the renamed column and data copied\n",
    "from the original DataFrame. We can verify that the original DataFrame did not have\n",
    "its column names modified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'symbol', 'open', 'close', 'low', 'high', 'volume'], dtype='object')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify the columns in the original did not change\n",
    "sp500.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To modify the DataFrame without making a copy, we can use the inplace=True\n",
    "parameter to .rename():\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'symbol', 'open', 'colosePrice', 'low', 'high', 'volume'], dtype='object')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this changes the column in-place\n",
    "sp500.rename(columns=\n",
    "{'close': 'colosePrice'},\n",
    "inplace=True)\n",
    "# we can see the column is changed\n",
    "sp500.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding and inserting columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                  date symbol        open  colosePrice         low  \\\n",
       "0  2016-01-05 00:00:00   WLTW  123.430000   125.839996  122.309998   \n",
       "1  2016-01-06 00:00:00   WLTW  125.239998   119.980003  119.940002   \n",
       "\n",
       "         high     volume  TwicePrice  \n",
       "0  126.250000  2163600.0  251.679992  \n",
       "1  125.540001  2386400.0  239.960006  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a copy\n",
    "copy = sp500.copy()\n",
    "# add a new column to the copy\n",
    "copy['TwicePrice'] = sp500.colosePrice * 2\n",
    "copy[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         open\n",
       "0  123.430000\n",
       "1  125.239998\n",
       "2  116.379997"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the first four rows and just the Price column\n",
    "rcopy = sp500[0:3][['open']].copy()\n",
    "rcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MMM      Is in the DataFrame\n",
       "MSFT    Not in the DataFrame\n",
       "dtype: object"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a new Series to merge as a column\n",
    "# one label exists in rcopy (MSFT), and MMM does not\n",
    "s = pd.Series(\n",
    "{'MMM': 'Is in the DataFrame',\n",
    " 'MSFT': 'Not in the DataFrame'} )\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         open Comment\n",
       "0  123.430000     NaN\n",
       "1  125.239998     NaN\n",
       "2  116.379997     NaN"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add rcopy into a column named 'Comment'\n",
    "rcopy['Comment'] = s\n",
    "rcopy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replacing the contents of a column\n",
    "In general, assignment of a Series to a column using the [] operator will either\n",
    "create a new column if the column does not already exist, or replace the contents\n",
    "of a column if it already exists. To demonstrate replacement, the following code\n",
    "replaces the Price column with the result of the multiplication, instead of creating\n",
    "a new column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                  date symbol        open  colosePrice         low  \\\n",
       "0  2016-01-05 00:00:00   WLTW  246.860000   125.839996  122.309998   \n",
       "1  2016-01-06 00:00:00   WLTW  250.479996   119.980003  119.940002   \n",
       "2  2016-01-07 00:00:00   WLTW  232.759994   114.949997  114.930000   \n",
       "3  2016-01-08 00:00:00   WLTW  230.960006   116.620003  113.500000   \n",
       "4  2016-01-11 00:00:00   WLTW  234.020004   114.970001  114.089996   \n",
       "\n",
       "         high     volume  \n",
       "0  126.250000  2163600.0  \n",
       "1  125.540001  2386400.0  \n",
       "2  119.739998  2489500.0  \n",
       "3  117.440002  2006300.0  \n",
       "4  117.330002  1408600.0  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copy = sp500.copy()\n",
    " # replace the Price column data with the new values\n",
    " # instead of adding a new column\n",
    "copy.open = sp500.open * 2\n",
    "copy[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deleting columns in a DataFrame\n",
    "Columns can be deleted from a DataFrame by using the del keyword, the pop(column)\n",
    "method of the DataFrame, or by calling the drop() method of the DataFrame."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The behavior of each of these differs slightly:\n",
    "• del will simply delete the Series from the DataFrame (in-place)\n",
    "• pop() will both delete the Series and return the Series as a result\n",
    "(also in-place)\n",
    "• drop(labels, axis=1) will return a new DataFrame with the column(s)\n",
    "removed (the original DataFrame object is not modified)\n",
    "The following code demonstrates using del to delete the BookValue column from a\n",
    "copy of the sp500 data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                  date symbol        open  colosePrice         low  \\\n",
       "0  2016-01-05 00:00:00   WLTW  123.430000   125.839996  122.309998   \n",
       "1  2016-01-06 00:00:00   WLTW  125.239998   119.980003  119.940002   \n",
       "\n",
       "         high     volume  \n",
       "0  126.250000  2163600.0  \n",
       "1  125.540001  2386400.0  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of using del to delete a column\n",
    " # make a copy of a subset of the data frame\n",
    "copy = sp500[:2].copy()\n",
    "copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                  date symbol        open         low        high     volume\n",
       "0  2016-01-05 00:00:00   WLTW  123.430000  122.309998  126.250000  2163600.0\n",
       "1  2016-01-06 00:00:00   WLTW  125.239998  119.940002  125.540001  2386400.0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete the BookValue column\n",
    " # deletion is in-place\n",
    "del(copy['colosePrice'])\n",
    "copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                  date symbol        open  colosePrice        high     volume\n",
       "0  2016-01-05 00:00:00   WLTW  123.430000   125.839996  126.250000  2163600.0\n",
       "1  2016-01-06 00:00:00   WLTW  125.239998   119.980003  125.540001  2386400.0"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of using pop to remove a column from a DataFrame\n",
    " # first make a copy of a subset of the data frame\n",
    "# pop works in-place\n",
    "copy = sp500[:2].copy()\n",
    "# this will remove Sector and return it as a series\n",
    "popped = copy.pop('low')\n",
    "# Sector column removed in-place\n",
    "copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    122.309998\n",
       "1    119.940002\n",
       "Name: low, dtype: float64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .drop() method can be used to remove both rows and columns. To use it to\n",
    "remove a column, specify axis=1:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                  date symbol        open         low        high     volume\n",
       "0  2016-01-05 00:00:00   WLTW  123.430000  122.309998  126.250000  2163600.0\n",
       "1  2016-01-06 00:00:00   WLTW  125.239998  119.940002  125.540001  2386400.0"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Example of using drop to remove a column\n",
    " # make a copy of a subset of the DataFrame\n",
    "copy = sp500[:2].copy()\n",
    " # this will return a new DataFrame with 'Sector' removed\n",
    " # the copy DataFrame is not modified\n",
    "afterdrop = copy.drop(['colosePrice'], axis = 1)\n",
    "afterdrop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding rows to a DataFrame"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Rows can be added to a DataFrame object via several different operations:\n",
    "• Appending a DataFrame to another\n",
    "• Concatenation of two DataFrame objects\n",
    "• Setting with enlargement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appending rows with .append()\n",
    "Appending is performed using the .append() method of the DataFrame. The\n",
    "process of appending returns a new DataFrame with the data from the original\n",
    "DataFrame added first, and the rows from the second. Appending does not perform\n",
    "alignment and can result in duplicate index values.<br>\n",
    "The following code demonstrates appending two DataFrame objects extracted\n",
    "from the sp500 data. The first DataFrame consists of rows 0, 1 and 2, and the\n",
    "second consists of rows 10, 11 and 2. Row 2 (with label ABBV) is included in\n",
    "both to demonstrate creation of duplicate index labels. The code is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                   date symbol        open  colosePrice         low  \\\n",
       "0   2016-01-05 00:00:00   WLTW  123.430000   125.839996  122.309998   \n",
       "1   2016-01-06 00:00:00   WLTW  125.239998   119.980003  119.940002   \n",
       "2   2016-01-07 00:00:00   WLTW  116.379997   114.949997  114.930000   \n",
       "10  2016-01-20 00:00:00   WLTW  109.059998   109.300003  108.320000   \n",
       "11  2016-01-21 00:00:00   WLTW  109.730003   110.000000  108.320000   \n",
       "2   2016-01-07 00:00:00   WLTW  116.379997   114.949997  114.930000   \n",
       "\n",
       "          high     volume  \n",
       "0   126.250000  2163600.0  \n",
       "1   125.540001  2386400.0  \n",
       "2   119.739998  2489500.0  \n",
       "10  111.599998  1653900.0  \n",
       "11  110.580002   944300.0  \n",
       "2   119.739998  2489500.0  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# copy the first three rows of sp500\n",
    "df1 = sp500.iloc[0:3].copy()\n",
    "# copy 10th and 11th rows\n",
    "df2 = sp500.iloc[[10, 11, 2]]\n",
    "# append df1 and df2\n",
    "appended = df1.append(df2)\n",
    "# the result is the rows of the first followed by\n",
    "# those of the second\n",
    "appended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The set of columns of the DataFrame objects being appended do not need to be the\n",
    "same. The resulting DataFrame will consist of the union of the columns in both and\n",
    "where either did not have a column, NaN will be used as the value. The following\n",
    "code demonstrates this by creating a third DataFrame using the same index as df1,\n",
    "but having a single column with a unique column name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   PER\n",
       "0  0.0\n",
       "1  0.0\n",
       "2  0.0"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DataFrame using df1.index and just a PER column\n",
    " # also a good example of using a scalar value\n",
    " # to initialize multiple rows\n",
    "df3 = pd.DataFrame(0.0,\n",
    "                   index=df1.index,\n",
    "                   columns=['PER'])\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# append df1 and df3\n",
    " # each has three rows, so 6 rows is the result\n",
    " # df1 had no PER column, so NaN for those rows\n",
    " # df3 had no BookValue, Price or Sector, so NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   PER  colosePrice                 date        high         low        open  \\\n",
       "0  NaN   125.839996  2016-01-05 00:00:00  126.250000  122.309998  123.430000   \n",
       "1  NaN   119.980003  2016-01-06 00:00:00  125.540001  119.940002  125.239998   \n",
       "2  NaN   114.949997  2016-01-07 00:00:00  119.739998  114.930000  116.379997   \n",
       "0  0.0          NaN                  NaN         NaN         NaN         NaN   \n",
       "1  0.0          NaN                  NaN         NaN         NaN         NaN   \n",
       "2  0.0          NaN                  NaN         NaN         NaN         NaN   \n",
       "\n",
       "  symbol     volume  \n",
       "0   WLTW  2163600.0  \n",
       "1   WLTW  2386400.0  \n",
       "2   WLTW  2489500.0  \n",
       "0    NaN        NaN  \n",
       "1    NaN        NaN  \n",
       "2    NaN        NaN  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.append(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To append without forcing the index to be taken from either DataFrame, you can\n",
    "use the ignore_index=True parameter. This is useful when the index values are\n",
    "not of significant meaning, and you just want concatenated data with sequentially\n",
    "increasing integers as indexes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   PER  colosePrice                 date        high         low        open  \\\n",
       "0  NaN   125.839996  2016-01-05 00:00:00  126.250000  122.309998  123.430000   \n",
       "1  NaN   119.980003  2016-01-06 00:00:00  125.540001  119.940002  125.239998   \n",
       "2  NaN   114.949997  2016-01-07 00:00:00  119.739998  114.930000  116.379997   \n",
       "3  0.0          NaN                  NaN         NaN         NaN         NaN   \n",
       "4  0.0          NaN                  NaN         NaN         NaN         NaN   \n",
       "5  0.0          NaN                  NaN         NaN         NaN         NaN   \n",
       "\n",
       "  symbol     volume  \n",
       "0   WLTW  2163600.0  \n",
       "1   WLTW  2386400.0  \n",
       "2   WLTW  2489500.0  \n",
       "3    NaN        NaN  \n",
       "4    NaN        NaN  \n",
       "5    NaN        NaN  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ignore index labels, create default index\n",
    "df1.append(df3, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenating DataFrame objects with pd.concat()\n",
    "A DataFrame can be concatenated to another using the pd.concat() function. This\n",
    "function functions similarly to the .append() method, but also adds the ability to\n",
    "specify an axis (appending can be row or column based), as well as being able to\n",
    "perform several join operations between the objects. Also, the function takes a list\n",
    "of pandas objects to concatenate, so you can concatenate more than two objects in a\n",
    "single call"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The default operation of pd.concat() on two DataFrame objects operates in the\n",
    "same way as the .append() method. This can be demonstrated by reconstructing the\n",
    "two datasets from the earlier example and concatenating them. This is shown in the\n",
    "following example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                   date symbol        open  colosePrice         low  \\\n",
       "0   2016-01-05 00:00:00   WLTW  123.430000   125.839996  122.309998   \n",
       "1   2016-01-06 00:00:00   WLTW  125.239998   119.980003  119.940002   \n",
       "2   2016-01-07 00:00:00   WLTW  116.379997   114.949997  114.930000   \n",
       "10  2016-01-20 00:00:00   WLTW  109.059998   109.300003  108.320000   \n",
       "11  2016-01-21 00:00:00   WLTW  109.730003   110.000000  108.320000   \n",
       "2   2016-01-07 00:00:00   WLTW  116.379997   114.949997  114.930000   \n",
       "\n",
       "          high     volume  \n",
       "0   126.250000  2163600.0  \n",
       "1   125.540001  2386400.0  \n",
       "2   119.739998  2489500.0  \n",
       "10  111.599998  1653900.0  \n",
       "11  110.580002   944300.0  \n",
       "2   119.739998  2489500.0  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# copy the first three rows of sp500\n",
    "\n",
    "df1 = sp500.iloc[0:3].copy()\n",
    "# copy 10th and 11th rows\n",
    "df2 = sp500.iloc[[10, 11, 2]]\n",
    "# pass them as a list\n",
    "pd.concat([df1, df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, pandas calculates the sorted union of distinct column names across all\n",
    "supplied objects and uses those as the columns, and then appends data along the\n",
    "rows for each object in the order given in the list.\n",
    "A slight variant of this example adds an additional column to one of the DataFrame\n",
    "objects and then performs the concatenation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                   date symbol        open  Foo  colosePrice     low  \\\n",
       "10  2016-01-20 00:00:00   WLTW  109.059998    0   109.300003  108.32   \n",
       "11  2016-01-21 00:00:00   WLTW  109.730003    0   110.000000  108.32   \n",
       "2   2016-01-07 00:00:00   WLTW  116.379997    0   114.949997  114.93   \n",
       "\n",
       "          high     volume  \n",
       "10  111.599998  1653900.0  \n",
       "11  110.580002   944300.0  \n",
       "2   119.739998  2489500.0  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # copy df2\n",
    "df2_2 = df2.copy()\n",
    "# add a column to df2_2 that is not in df1\n",
    "df2_2.insert(3, 'Foo', pd.Series(0, index=df2.index))\n",
    "# see what it looks like\n",
    "df2_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    Foo  colosePrice                 date        high         low        open  \\\n",
       "0   NaN   125.839996  2016-01-05 00:00:00  126.250000  122.309998  123.430000   \n",
       "1   NaN   119.980003  2016-01-06 00:00:00  125.540001  119.940002  125.239998   \n",
       "2   NaN   114.949997  2016-01-07 00:00:00  119.739998  114.930000  116.379997   \n",
       "10  0.0   109.300003  2016-01-20 00:00:00  111.599998  108.320000  109.059998   \n",
       "11  0.0   110.000000  2016-01-21 00:00:00  110.580002  108.320000  109.730003   \n",
       "2   0.0   114.949997  2016-01-07 00:00:00  119.739998  114.930000  116.379997   \n",
       "\n",
       "   symbol     volume  \n",
       "0    WLTW  2163600.0  \n",
       "1    WLTW  2386400.0  \n",
       "2    WLTW  2489500.0  \n",
       "10   WLTW  1653900.0  \n",
       "11   WLTW   944300.0  \n",
       "2    WLTW  2489500.0  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now concatenate\n",
    "pd.concat([df1, df2_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplicate index labels still result, as the rows are copied verbatim from the source\n",
    "objects. However, note the NaN values in the rows originating from df1, since it does\n",
    "not have a Foo column.\n",
    "Using the keys parameter, it is possible to differentiate the pandas objects from\n",
    "which the rows originated. The following code adds a level to the index which\n",
    "represents the source object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        Foo  colosePrice                 date        high         low  \\\n",
       "df1 0   NaN   125.839996  2016-01-05 00:00:00  126.250000  122.309998   \n",
       "    1   NaN   119.980003  2016-01-06 00:00:00  125.540001  119.940002   \n",
       "    2   NaN   114.949997  2016-01-07 00:00:00  119.739998  114.930000   \n",
       "df2 10  0.0   109.300003  2016-01-20 00:00:00  111.599998  108.320000   \n",
       "    11  0.0   110.000000  2016-01-21 00:00:00  110.580002  108.320000   \n",
       "    2   0.0   114.949997  2016-01-07 00:00:00  119.739998  114.930000   \n",
       "\n",
       "              open symbol     volume  \n",
       "df1 0   123.430000   WLTW  2163600.0  \n",
       "    1   125.239998   WLTW  2386400.0  \n",
       "    2   116.379997   WLTW  2489500.0  \n",
       "df2 10  109.059998   WLTW  1653900.0  \n",
       "    11  109.730003   WLTW   944300.0  \n",
       "    2   116.379997   WLTW  2489500.0  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify keys\n",
    "r = pd.concat([df1, df2_2], keys=['df1', 'df2'])\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change the axis of the concatenation to work along the columns by specifying\n",
    "axis=1, which will calculate the sorted union of the distinct index labels from the\n",
    "rows and then append columns and their data from the specified objects.\n",
    "To demonstrate, the following splits the sp500 data into two DataFrame objects, each\n",
    "with a different set of columns, and then concatenates along axis=1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         open        high\n",
       "0  123.430000  126.250000\n",
       "1  125.239998  125.540001\n",
       "2  116.379997  119.739998"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first three rows, columns 0 and 1\n",
    "df3 = sp500[:3][['open','high']]\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         open\n",
       "0  123.430000\n",
       "1  125.239998\n",
       "2  116.379997"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first three rows, column 2\n",
    "df4 = sp500[:3][[\"open\"]]\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         open        high        open\n",
       "0  123.430000  126.250000  123.430000\n",
       "1  125.239998  125.540001  125.239998\n",
       "2  116.379997  119.739998  116.379997"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([df3, df4], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further examine this operation by adding a column to the second\n",
    "DataFrame that has a duplicate name to a column in the first. The result will\n",
    "have duplicate columns, as the columns are blindly appended without regard\n",
    "to already existing columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         open  Sector\n",
       "0  123.430000       1\n",
       "1  125.239998       1\n",
       "2  116.379997       1"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # make a copy of df4\n",
    "df4_2 = df4.copy()\n",
    "# add a column to df4_2, that is also in df3\n",
    "df4_2.insert(1, 'Sector', pd.Series(1, index=df4_2.index))\n",
    "df4_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         open        high        open  Sector\n",
       "0  123.430000  126.250000  123.430000       1\n",
       "1  125.239998  125.540001  125.239998       1\n",
       "2  116.379997  119.739998  116.379997       1"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# demonstrate duplicate columns\n",
    "pd.concat([df3, df4_2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be very specific, pandas is performing an outer join along the labels of the specified\n",
    "axis. An inner join can be specified using the join='inner' parameter, which\n",
    "changes the operation from being a sorted union of distinct labels to the distinct\n",
    "values of the intersection of the labels. To demonstrate, the following selects two\n",
    "subsets of the financial data with one row in common and performs an inner join:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         open        high\n",
       "0  123.430000  126.250000\n",
       "1  125.239998  125.540001\n",
       "2  116.379997  119.739998"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first three rows and first two columns\n",
    "df5 = sp500[:3][[\"open\",\"high\"]]\n",
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         high         low\n",
       "2  119.739998  114.930000\n",
       "3  117.440002  113.500000\n",
       "4  117.330002  114.089996"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# row 2 through 4 and first two columns\n",
    "df6 = sp500[2:5][[\"high\",'low']]\n",
    "df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         open        high        high     low\n",
       "2  116.379997  119.739998  119.739998  114.93"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inner join on index labels will return in only one row\n",
    "pd.concat([df5, df6], join='inner', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding rows (and columns) via setting with\n",
    "enlargementAdding rows (and columns) via setting with\n",
    "enlargement<br><br>\n",
    "Rows can also be added to a DataFrame through the .loc property. This technique\n",
    "is referred to as setting with enlargement. The parameter for .loc specifies the\n",
    "index label where the row is to be placed. If the label does not exist, the values are\n",
    "appended to the DataFrame using the given index label. If it does exist, then the\n",
    "values in the specified row are replaced.\n",
    "The following example takes a subset of sp500 and adds a row with the label FOO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                  date symbol        open  colosePrice         low  \\\n",
       "0  2016-01-05 00:00:00   WLTW  123.430000   125.839996  122.309998   \n",
       "1  2016-01-06 00:00:00   WLTW  125.239998   119.980003  119.940002   \n",
       "2  2016-01-07 00:00:00   WLTW  116.379997   114.949997  114.930000   \n",
       "\n",
       "         high     volume         FOO  \n",
       "0  126.250000  2163600.0  the sector  \n",
       "1  125.540001  2386400.0         100  \n",
       "2  119.739998  2489500.0         110  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a small subset of the sp500\n",
    "# make sure to copy the slice to make a copy\n",
    "ss = sp500[:3].copy()\n",
    "# create a new row with index label FOO\n",
    "# and assign some values to the columns via a list\n",
    "ss['FOO'] = ['the sector', 100, 110]\n",
    "ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to add columns in this manner. The following code demonstrates\n",
    "by adding a new column to a subset of sp500 using .loc. Note that to accomplish\n",
    "this, we use the colon in the rows' position to select all rows to be included to add\n",
    "the new column and value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                  date symbol        open  colosePrice         low  \\\n",
       "0  2016-01-05 00:00:00   WLTW  123.430000   125.839996  122.309998   \n",
       "1  2016-01-06 00:00:00   WLTW  125.239998   119.980003  119.940002   \n",
       "2  2016-01-07 00:00:00   WLTW  116.379997   114.949997  114.930000   \n",
       "\n",
       "         high     volume  PER  \n",
       "0  126.250000  2163600.0    0  \n",
       "1  125.540001  2386400.0    0  \n",
       "2  119.739998  2489500.0    0  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# copy of subset / slice\n",
    "ss = sp500[:3].copy()\n",
    " # add the new column initialized to 0\n",
    "ss.loc[:,'PER'] = 0\n",
    " # take a look at the results\n",
    "ss"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Removing rows from a DataFrame\n",
    "Removing rows from a DataFrame object is normally performed using one of\n",
    "three techniques:\n",
    "• Using the .drop() method\n",
    "• Boolean selection\n",
    "• Selection using a slice\n",
    "Technically, only the .drop() method removes rows in-place on the source object.\n",
    "The other techniques either create a copy without specific rows, or a view into the\n",
    "rows that are not to be dropped. Details of each are given in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
